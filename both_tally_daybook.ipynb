{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkar-salunke/accounts/blob/main/both_tally_daybook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR0J6hvdZQVT",
        "outputId": "e5db506a-7b80-4027-b65b-cbf472fdd749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "WoW6xYOdZkaD"
      },
      "outputs": [],
      "source": [
        "# prompt: list of files in the path in python\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "# Function to recursively extract text and sub-elements from a node\n",
        "def extract_element_data(element):\n",
        "    element_data = {}\n",
        "    # If the element has no children, return its text\n",
        "    if len(element) == 0:\n",
        "        return element.text\n",
        "\n",
        "    # Otherwise, go deeper into the element's children\n",
        "    for child in element:\n",
        "        # Recursively extract data for each child element\n",
        "        child_data = extract_element_data(child)\n",
        "        element_data[child.tag] = child_data\n",
        "    return element_data\n",
        "\n",
        "# Function to flatten nested dictionaries (e.g., 'amount': {'P': {'T': 100}})\n",
        "def flatten_dict(d, parent_key='', sep='.'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "# Function to recursively search for all 'voucher' elements and extract their data\n",
        "def find_all_vouchers(element, voucher_tag=\"VOUCHER\"):\n",
        "    vouchers = []\n",
        "\n",
        "    # If the current element is a 'voucher', collect its data\n",
        "    if element.tag == voucher_tag:\n",
        "        voucher_dict = {}\n",
        "        # Collect data for all child elements within the voucher\n",
        "        for sub_child in element:\n",
        "            extracted_data = extract_element_data(sub_child)\n",
        "            voucher_dict[sub_child.tag] = extracted_data\n",
        "\n",
        "        # Flatten the voucher dictionary to handle nested elements\n",
        "        voucher_dict_flattened = flatten_dict(voucher_dict)\n",
        "        vouchers.append(voucher_dict_flattened)\n",
        "\n",
        "    # Recursively search in all children of the current element\n",
        "    for child in element:\n",
        "        vouchers.extend(find_all_vouchers(child, voucher_tag))\n",
        "\n",
        "    return vouchers\n",
        "\n",
        "# Function to convert the XML to a DataFrame by extracting all vouchers\n",
        "def xml_to_dataframe_vouchers(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Find all 'voucher' elements in the XML\n",
        "    voucher_list = find_all_vouchers(root)\n",
        "\n",
        "    # Create a DataFrame from the list of voucher dictionaries\n",
        "    df = pd.DataFrame(voucher_list)\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_bank_match_soe = pd.read_excel(\"/content/drive/My Drive/2024_25/overview_account.xlsx\",sheet_name='SOE 2024-25')\n",
        "df_bank_match_overview_soe = df_bank_match_soe.groupby('comment').agg({'Debit':'sum','Credit':'sum'}).reset_index().rename(columns={'comment':'party_name'})\n",
        "\n",
        "df_bank_match_me = pd.read_excel(\"/content/drive/My Drive/2024_25/overview_account.xlsx\",sheet_name='ME 2024-25')\n",
        "df_bank_match_overview_me = df_bank_match_me.groupby('comment').agg({'Debit':'sum','Credit':'sum'}).reset_index().rename(columns={'comment':'party_name'})"
      ],
      "metadata": {
        "id": "cgmJWFGXQc4v"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "3AiRkkfwrgPs"
      },
      "outputs": [],
      "source": [
        "files_soe = os.listdir('/content/drive/My Drive/2024_25/SOE/')\n",
        "files_me = os.listdir('/content/drive/My Drive/2024_25/ME/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "req_cols = ['DATE','GUID','VOUCHERTYPENAME','PARTYLEDGERNAME','PARTYGSTIN','CLASSNAME','PARTYNAME','CONSIGNEEGSTIN','PERSISTEDVIEW','ALLINVENTORYENTRIES.LIST.STOCKITEMNAME','ALLINVENTORYENTRIES.LIST.RATE','ALLINVENTORYENTRIES.LIST.AMOUNT','LEDGERENTRIES.LIST.VATEXPAMOUNT','LEDGERENTRIES.LIST.AMOUNT','ALLINVENTORYENTRIES.LIST.ACTUALQTY','ALLINVENTORYENTRIES.LIST.BILLEDQTY','ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.AMOUNT','ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.ACTUALQTY','ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.BILLEDQTY','ALLINVENTORYENTRIES.LIST.ACCOUNTINGALLOCATIONS.LIST.LEDGERNAME','ALLINVENTORYENTRIES.LIST.ACCOUNTINGALLOCATIONS.LIST.AMOUNT','LEDGERENTRIES.LIST.LEDGERNAME','LEDGERENTRIES.LIST.AMOUNT','LEDGERENTRIES.LIST.VATEXPAMOUNT','INVOICEORDERLIST.LIST.BASICORDERDATE','INVOICEORDERLIST.LIST.BASICPURCHASEORDERNO','REFERENCEDATE','REFERENCE','CONSIGNEEPINNUMBER','ALLINVENTORYENTRIES.LIST.ACCOUNTINGALLOCATIONS.LIST.GSTOVRDNNATURE','ALLINVENTORYENTRIES.LIST.ACCOUNTINGALLOCATIONS.LIST.RATEDETAILS.LIST.GSTRATEDUTYHEAD','ALLLEDGERENTRIES.LIST.OLDAUDITENTRYIDS.LIST.OLDAUDITENTRYIDS','ALLLEDGERENTRIES.LIST.LEDGERNAME','ALLLEDGERENTRIES.LIST.AMOUNT','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.DATE','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.INSTRUMENTDATE','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.NAME','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.TRANSACTIONTYPE','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.PAYMENTFAVOURING','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.CHEQUECROSSCOMMENT','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.UNIQUEREFERENCENUMBER','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.STATUS','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.PAYMENTMODE','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.BANKPARTYNAME','ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.AMOUNT','NARRATION']\n",
        "df_raw_collect_soe = pd.DataFrame()\n",
        "for xt in files_soe:\n",
        "  path_ct = '/content/drive/My Drive/2024_25/SOE/'\n",
        "  xml_file = path_ct+xt\n",
        "  df_raw_f = xml_to_dataframe_vouchers(xml_file)[req_cols].reset_index(drop=True)\n",
        "  df_raw_f_ref = pd.DataFrame(df_raw_f)\n",
        "  df_raw_collect_soe = pd.concat([df_raw_collect_soe, df_raw_f_ref]).reset_index(drop=True)\n",
        "\n",
        "df_raw_soe = df_raw_collect_soe.drop_duplicates(subset='GUID', keep='first').reset_index(drop=True)\n",
        "df_raw_soe['date_column'] = pd.to_datetime(df_raw_soe['DATE'], format='%Y%m%d')\n",
        "df_raw_soe = df_raw_soe.sort_values('date_column')\n",
        "\n",
        "df_raw_collect_me = pd.DataFrame()\n",
        "for xt in files_me:\n",
        "  path_ct = '/content/drive/My Drive/2024_25/ME/'\n",
        "  xml_file = path_ct+xt\n",
        "  df_raw_f = xml_to_dataframe_vouchers(xml_file)[req_cols].reset_index(drop=True)\n",
        "  df_raw_f_ref = pd.DataFrame(df_raw_f)\n",
        "  df_raw_collect_me = pd.concat([df_raw_collect_me, df_raw_f_ref]).reset_index(drop=True)\n",
        "\n",
        "df_raw_me = df_raw_collect_me.drop_duplicates(subset='GUID', keep='first').reset_index(drop=True)\n",
        "df_raw_me['date_column'] = pd.to_datetime(df_raw_me['DATE'], format='%Y%m%d')\n",
        "df_raw_me = df_raw_me.sort_values('date_column')"
      ],
      "metadata": {
        "id": "K5L6GoEDvbmg"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_req_fields(df):\n",
        "  df['party_name'] = df[['PARTYNAME', 'ALLLEDGERENTRIES.LIST.BANKALLOCATIONS.LIST.PAYMENTFAVOURING', 'ALLLEDGERENTRIES.LIST.LEDGERNAME']].bfill(axis=1).iloc[:, 0]\n",
        "  df['tax_out_1'] = df[['LEDGERENTRIES.LIST.AMOUNT']].bfill(axis=1).iloc[:, 0]\n",
        "  df['tax_out_2'] = df[['LEDGERENTRIES.LIST.VATEXPAMOUNT']].bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "  # df[''] = df[''].astype(float)\n",
        "  df['ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.AMOUNT'] = df['ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.AMOUNT'].astype(float)\n",
        "  df['tax_out_1'] = df['tax_out_1'].astype(float)\n",
        "  df['tax_out_2'] = df['tax_out_2'].astype(float)\n",
        "\n",
        "  df['amount_coalesce'] = df[['ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.AMOUNT', 'ALLLEDGERENTRIES.LIST.AMOUNT']].bfill(axis=1).iloc[:, 0]\n",
        "  df['amount_coalesce'] = df['amount_coalesce'].astype(float)\n",
        "  df['tax_coalesce'] = df[['tax_out_1', 'tax_out_2']].sum(axis=1)\n",
        "  df['total_coalesce'] = df[['tax_coalesce', 'amount_coalesce']].sum(axis=1)\n",
        "\n",
        "  df = df[['party_name','total_coalesce','amount_coalesce','tax_coalesce','tax_out_1','tax_out_2']+req_cols]\n",
        "  return df"
      ],
      "metadata": {
        "id": "XNtRcjgUAF05"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_me = add_req_fields(df_raw_me)\n",
        "df_me['DATE'] = pd.to_datetime(df_me['DATE'])\n",
        "df_me['month_of_day'] =df_me['DATE'].dt.month\n",
        "df_me['quarter_of_day'] =df_me['DATE'].dt.quarter\n",
        "\n",
        "df_soe = add_req_fields(df_raw_soe)\n",
        "df_soe['DATE'] = pd.to_datetime(df_soe['DATE'])\n",
        "df_soe['month_of_day'] =df_soe['DATE'].dt.month\n",
        "df_soe['quarter_of_day'] =df_soe['DATE'].dt.quarter\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SrhORyvhe1v",
        "outputId": "7d4b265b-363c-4d30-e6bb-0446de5fa02b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-bc721fa37c88>:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['amount_coalesce'] = df[['ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.AMOUNT', 'ALLLEDGERENTRIES.LIST.AMOUNT']].bfill(axis=1).iloc[:, 0]\n",
            "<ipython-input-81-9f1d164c3bb6>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_me['DATE'] = pd.to_datetime(df_me['DATE'])\n",
            "<ipython-input-81-9f1d164c3bb6>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_me['month_of_day'] =df_me['DATE'].dt.month\n",
            "<ipython-input-81-9f1d164c3bb6>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_me['quarter_of_day'] =df_me['DATE'].dt.quarter\n",
            "<ipython-input-80-bc721fa37c88>:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['amount_coalesce'] = df[['ALLINVENTORYENTRIES.LIST.BATCHALLOCATIONS.LIST.AMOUNT', 'ALLLEDGERENTRIES.LIST.AMOUNT']].bfill(axis=1).iloc[:, 0]\n",
            "<ipython-input-81-9f1d164c3bb6>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_soe['DATE'] = pd.to_datetime(df_soe['DATE'])\n",
            "<ipython-input-81-9f1d164c3bb6>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_soe['month_of_day'] =df_soe['DATE'].dt.month\n",
            "<ipython-input-81-9f1d164c3bb6>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_soe['quarter_of_day'] =df_soe['DATE'].dt.quarter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_soe['total_coalesce'] = df_soe['total_coalesce'].astype('float')\n",
        "df_summary_all_soe = pd.pivot_table(df_soe, values='total_coalesce', index='party_name', columns='VOUCHERTYPENAME', aggfunc='sum').reset_index()\n",
        "df_summary_all_soe['balance'] = 0\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='TIRUMALA MULTI TECHNOLOGIES'].index,'balance'] = 467646\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='SAVITA AUTO INDUSTRIES (N)'].index,'balance'] = 195853\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='PATIL-N-PATIL ENGINEERING\t'].index,'balance'] = 55660\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='NANDAMURI TECHNOS'].index,'balance'] = 35255\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='V.S.AUTO TECH PRIVATE LIMITED'].index,'balance'] = 4699956\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='A.B.ENGINEERING WORKS'].index,'balance'] = 11598\n",
        "\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='AKAR INDUSTRY'].index,'balance'] = 317899\n",
        "\n",
        "df_summary_all_soe.loc[df_summary_all_soe[df_summary_all_soe['party_name']=='SURYA AUTO INDUSTRIES'].index,'balance'] = 23165\n",
        "\n",
        "\n",
        "df_summary_all_soe = df_summary_all_soe.fillna(0)\n",
        "df_summary_all_soe['total_due']=df_summary_all_soe[['Contra', 'Credit Note', 'DELIVERY CHALLAN', 'Debit Note',\n",
        "       'Journal', 'Payment', 'Purchase', 'Receipt', 'SALES GST', 'balance']].sum(axis=1)\n",
        "\n",
        "df_me['total_coalesce'] = df_me['total_coalesce'].astype('float')\n",
        "df_summary_all_me = pd.pivot_table(df_me, values='total_coalesce', index='party_name', columns='VOUCHERTYPENAME', aggfunc='sum').reset_index()\n",
        "df_summary_all_me['balance'] = 0\n",
        "df_summary_all_me = df_summary_all_me.fillna(0)\n",
        "df_summary_all_me['total_due']=df_summary_all_me[[ 'Contra','Delivery Note','Debit Note','Sales Gst', 'Journal', 'Payment', 'Purchase', 'Receipt','Sales Gst','balance']].sum(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB4ofqqrIF3K",
        "outputId": "efdfa432-3d0d-407d-a076-568f016cd10d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-c45c9a62cf25>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_soe['total_coalesce'] = df_soe['total_coalesce'].astype('float')\n",
            "<ipython-input-99-c45c9a62cf25>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_me['total_coalesce'] = df_me['total_coalesce'].astype('float')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# terms = ['SHREE SAI ENGINEERING', 'XYZ COMPANY'] # Function to match and replace terms def match_and_replace(transaction): for term in terms: if term.replace(\" \", \"\").lower() in transaction.replace(\" \", \"\").lower(): return term return transaction # Or return a default value if no match is found # Apply function to the column df['renamed_column'] = df['transaction_column'"
      ],
      "metadata": {
        "id": "_dLjrfODnsqa"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_bank_match\n",
        "df_summary_all_bank_soe = pd.merge(df_summary_all_soe,df_bank_match_overview_soe,on=['party_name'],how='outer')\n",
        "df_summary_all_bank_soe = df_summary_all_bank_soe.sort_values(by='SALES GST', key=lambda x: x.isna()).reset_index(drop=True).sort_values(by='SALES GST', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# df_bank_match\n",
        "df_summary_all_bank_me = pd.merge(df_summary_all_me,df_bank_match_overview_me,on=['party_name'],how='outer')\n",
        "df_summary_all_bank_me = df_summary_all_bank_me.sort_values(by='Sales Gst', key=lambda x: x.isna()).reset_index(drop=True).sort_values(by='Sales Gst', ascending=False).reset_index(drop=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "wHV_d94IRwaE"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_sales = df_soe[df_soe['VOUCHERTYPENAME']=='SALES GST'].reset_index(drop=True)\n",
        "# df_sales = df_sales.rename(columns={'ALLINVENTORYENTRIES.LIST.AMOUNT': 'sales_amount'})\n",
        "df_sales_g = df_sales.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"total_sales\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "date_45d = max(df_sales['DATE']) - pd.Timedelta(days=45)\n",
        "due45 = df_sales[df_sales['DATE'] > date_45d]\n",
        "due45_g = due45.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"Sale_45d\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "\n",
        "date_60d = max(df_sales['DATE']) - pd.Timedelta(days=60)\n",
        "due60 = df_sales[df_sales['DATE'] > date_60d]\n",
        "due60_g = due60.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"Sale_60d\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "\n",
        "date_90d = max(df_sales['DATE']) - pd.Timedelta(days=90)\n",
        "due90 = df_sales[df_sales['DATE'] > date_90d]\n",
        "due90_g = due90.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"Sale_90d\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "\n",
        "# df_summary_all_bank_soe_due = pd.merge(df_summary_all_bank_soe,df_sales_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_soe_due = pd.merge(df_summary_all_bank_soe,due45_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_soe_due = pd.merge(df_summary_all_bank_soe_due,due60_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_soe_due = pd.merge(df_summary_all_bank_soe_due,due90_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_soe_due = df_summary_all_bank_soe_due.fillna(0)\n",
        "df_summary_all_bank_soe_due['true_credit'] = df_summary_all_bank_soe_due['Credit']-df_summary_all_bank_soe_due['balance']\n",
        "# df_summary_all_bank_soe_due"
      ],
      "metadata": {
        "id": "cVvXx6dpooKa"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_sales = df_me[df_me['VOUCHERTYPENAME']=='Sales Gst'].reset_index(drop=True)\n",
        "# df_sales = df_sales.rename(columns={'ALLINVENTORYENTRIES.LIST.AMOUNT': 'sales_amount'})\n",
        "df_sales_g = df_sales.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"total_sales\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "date_45d = max(df_sales['DATE']) - pd.Timedelta(days=45)\n",
        "due45 = df_sales[df_sales['DATE'] > date_45d]\n",
        "due45_g = due45.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"Sale_45d\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "\n",
        "date_60d = max(df_sales['DATE']) - pd.Timedelta(days=60)\n",
        "due60 = df_sales[df_sales['DATE'] > date_60d]\n",
        "due60_g = due60.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"Sale_60d\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "\n",
        "date_90d = max(df_sales['DATE']) - pd.Timedelta(days=90)\n",
        "due90 = df_sales[df_sales['DATE'] > date_90d]\n",
        "due90_g = due90.groupby(['PARTYLEDGERNAME'])['total_coalesce'].sum().reset_index().rename(columns={\"total_coalesce\":\"Sale_90d\",\"PARTYLEDGERNAME\":\"party_name\"})\n",
        "\n",
        "# df_summary_all_bank_me_due = pd.merge(df_summary_all_bank_me,df_sales_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_me_due = pd.merge(df_summary_all_bank_me,due45_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_me_due = pd.merge(df_summary_all_bank_me_due,due60_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_me_due = pd.merge(df_summary_all_bank_me_due,due90_g,on=['party_name'],how='left')\n",
        "df_summary_all_bank_me_due = df_summary_all_bank_me_due.fillna(0)\n",
        "df_summary_all_bank_me_due['true_credit'] = df_summary_all_bank_me_due['Credit']-df_summary_all_bank_me_due['balance']\n",
        "# df_summary_all_bank_me_due"
      ],
      "metadata": {
        "id": "vA1xladDj7p7"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "columns_to_round_soe = ['Contra', 'Credit Note', 'DELIVERY CHALLAN', 'Debit Note','Journal', 'Payment', 'Purchase', 'Receipt', 'SALES GST', 'balance','total_due', 'Debit', 'Credit','Sale_45d','Sale_60d','Sale_90d']\n",
        "df_summary_all_bank_soe_due[columns_to_round_soe] = df_summary_all_bank_soe_due[columns_to_round_soe].apply(np.ceil)\n",
        "\n",
        "columns_to_round_me = ['Contra', 'Debit Note', 'Delivery Note', 'Journal','Payment', 'Purchase', 'Receipt', 'Sales Gst', 'balance', 'total_due','Debit', 'Credit','Sale_45d','Sale_60d','Sale_90d']\n",
        "df_summary_all_bank_me_due[columns_to_round_me] = df_summary_all_bank_me_due[columns_to_round_me].apply(np.ceil)"
      ],
      "metadata": {
        "id": "hPkVY1833bD1"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from datetime import datetime\n",
        "\n",
        "# Define file path and sheet name\n",
        "file_path = \"/content/drive/My Drive/2024_25/daybook.xlsx\"\n",
        "original_sheet_name = \"SOE_overview\"\n",
        "\n",
        "# Get the current date to use in the renamed sheet name\n",
        "date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "renamed_sheet_name = f\"{original_sheet_name}_{date_str}\"\n",
        "\n",
        "# Load the workbook and rename the sheet if it exists\n",
        "workbook = load_workbook(file_path)\n",
        "\n",
        "if original_sheet_name in workbook.sheetnames:\n",
        "    # Rename the existing sheet\n",
        "    sheet = workbook[original_sheet_name]\n",
        "    sheet.title = renamed_sheet_name\n",
        "    sheet.sheet_state = 'hidden'  # Hide the sheet\n",
        "\n",
        "# Save the workbook after renaming and hiding the sheet\n",
        "workbook.save(file_path)\n",
        "\n",
        "# Write the new data to the original sheet name\n",
        "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
        "    df_summary_all_bank_soe_due.to_excel(writer, sheet_name=original_sheet_name, index=False)\n"
      ],
      "metadata": {
        "id": "Zzg1eHNKzh6l"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sheet_name = \"SOE_raw_xml\"\n",
        "\n",
        "# Get the current date to use in the renamed sheet name\n",
        "date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "renamed_sheet_name = f\"{original_sheet_name}_{date_str}\"\n",
        "\n",
        "# Load the workbook and rename the sheet if it exists\n",
        "workbook = load_workbook(file_path)\n",
        "\n",
        "if original_sheet_name in workbook.sheetnames:\n",
        "    # Rename the existing sheet\n",
        "    sheet = workbook[original_sheet_name]\n",
        "    sheet.title = renamed_sheet_name\n",
        "    sheet.sheet_state = 'hidden'  # Hide the sheet\n",
        "\n",
        "# Save the workbook after renaming and hiding the sheet\n",
        "workbook.save(file_path)\n",
        "\n",
        "# Write the new data to the original sheet name\n",
        "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
        "    df_soe.to_excel(writer, sheet_name=original_sheet_name, index=False)"
      ],
      "metadata": {
        "id": "w3R6wL5x0O62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sheet_name = \"ME_overview\"\n",
        "\n",
        "# Get the current date to use in the renamed sheet name\n",
        "date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "renamed_sheet_name = f\"{original_sheet_name}_{date_str}\"\n",
        "\n",
        "# Load the workbook and rename the sheet if it exists\n",
        "workbook = load_workbook(file_path)\n",
        "\n",
        "if original_sheet_name in workbook.sheetnames:\n",
        "    # Rename the existing sheet\n",
        "    sheet = workbook[original_sheet_name]\n",
        "    sheet.title = renamed_sheet_name\n",
        "    sheet.sheet_state = 'hidden'  # Hide the sheet\n",
        "\n",
        "# Save the workbook after renaming and hiding the sheet\n",
        "workbook.save(file_path)\n",
        "\n",
        "# Write the new data to the original sheet name\n",
        "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
        "    df_summary_all_bank_me_due.to_excel(writer, sheet_name=original_sheet_name, index=False)"
      ],
      "metadata": {
        "id": "cywYr9LO0Y76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sheet_name = \"ME_raw_xml\"\n",
        "\n",
        "# Get the current date to use in the renamed sheet name\n",
        "date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "renamed_sheet_name = f\"{original_sheet_name}_{date_str}\"\n",
        "\n",
        "# Load the workbook and rename the sheet if it exists\n",
        "workbook = load_workbook(file_path)\n",
        "\n",
        "if original_sheet_name in workbook.sheetnames:\n",
        "    # Rename the existing sheet\n",
        "    sheet = workbook[original_sheet_name]\n",
        "    sheet.title = renamed_sheet_name\n",
        "    sheet.sheet_state = 'hidden'  # Hide the sheet\n",
        "\n",
        "# Save the workbook after renaming and hiding the sheet\n",
        "workbook.save(file_path)\n",
        "\n",
        "# Write the new data to the original sheet name\n",
        "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
        "    df_me.to_excel(writer, sheet_name=original_sheet_name, index=False)"
      ],
      "metadata": {
        "id": "Cdtw_s6B0dQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sheet_name = \"SOE24-25\"\n",
        "\n",
        "# Get the current date to use in the renamed sheet name\n",
        "date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "renamed_sheet_name = f\"{original_sheet_name}_{date_str}\"\n",
        "\n",
        "# Load the workbook and rename the sheet if it exists\n",
        "workbook = load_workbook(file_path)\n",
        "\n",
        "if original_sheet_name in workbook.sheetnames:\n",
        "    # Rename the existing sheet\n",
        "    sheet = workbook[original_sheet_name]\n",
        "    sheet.title = renamed_sheet_name\n",
        "    sheet.sheet_state = 'hidden'  # Hide the sheet\n",
        "\n",
        "# Save the workbook after renaming and hiding the sheet\n",
        "workbook.save(file_path)\n",
        "\n",
        "# Write the new data to the original sheet name\n",
        "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
        "    df_bank_match_soe.to_excel(writer, sheet_name=original_sheet_name, index=False)\n"
      ],
      "metadata": {
        "id": "wL6VQgTX003X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sheet_name = \"ME24-25\"\n",
        "\n",
        "# Get the current date to use in the renamed sheet name\n",
        "date_str = datetime.now().strftime(\"%Y_%m_%d\")\n",
        "renamed_sheet_name = f\"{original_sheet_name}_{date_str}\"\n",
        "\n",
        "# Load the workbook and rename the sheet if it exists\n",
        "workbook = load_workbook(file_path)\n",
        "\n",
        "if original_sheet_name in workbook.sheetnames:\n",
        "    # Rename the existing sheet\n",
        "    sheet = workbook[original_sheet_name]\n",
        "    sheet.title = renamed_sheet_name\n",
        "    sheet.sheet_state = 'hidden'  # Hide the sheet\n",
        "\n",
        "# Save the workbook after renaming and hiding the sheet\n",
        "workbook.save(file_path)\n",
        "\n",
        "# Write the new data to the original sheet name\n",
        "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
        "    df_bank_match_me.to_excel(writer, sheet_name=original_sheet_name, index=False)\n"
      ],
      "metadata": {
        "id": "gT4-ol_G09am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary_all_bank_soe_due.to_excel(\"/content/drive/My Drive/2024_25/daybook.xlsx\",sheet_name='SOE_overview')\n",
        "\n",
        "with pd.ExcelWriter(\"/content/drive/My Drive/2024_25/daybook.xlsx\",mode='a') as writer:\n",
        "  df_soe.to_excel(writer,sheet_name='SOE_raw_xml')\n",
        "with pd.ExcelWriter(\"/content/drive/My Drive/2024_25/daybook.xlsx\",mode='a') as writer:\n",
        "  df_summary_all_bank_me_due.to_excel(writer,sheet_name='ME_overview')\n",
        "with pd.ExcelWriter(\"/content/drive/My Drive/2024_25/daybook.xlsx\",mode='a') as writer:\n",
        "  df_me.to_excel(writer,sheet_name='ME_raw_xml')"
      ],
      "metadata": {
        "id": "z2pbRc4CEuV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_soe"
      ],
      "metadata": {
        "id": "-5vbgZEIt2Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_HZlR-_t762"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}